<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="generator" content="rustdoc">
    <meta name="description" content="Source to the Rust file `/home/travis/.cargo/registry/src/github.com-88ac128001ac3a9a/collenchyma-blas-0.2.0/src/plugin.rs`.">
    <meta name="keywords" content="rust, rustlang, rust-lang">

    <title>plugin.rs.html -- source</title>

    <link rel="stylesheet" type="text/css" href="../../rustdoc.css">
    <link rel="stylesheet" type="text/css" href="../../main.css">

    
    
</head>
<body class="rustdoc">
    <!--[if lte IE 8]>
    <div class="warning">
        This old browser is unsupported and will most likely display funky
        things.
    </div>
    <![endif]-->

    

    <nav class="sidebar">
        
        
    </nav>

    <nav class="sub">
        <form class="search-form js-only">
            <div class="search-container">
                <input class="search-input" name="search"
                       autocomplete="off"
                       placeholder="Click or press ‘S’ to search, ‘?’ for more options…"
                       type="search">
            </div>
        </form>
    </nav>

    <section id='main' class="content source"><pre class="line-numbers"><span id="1">  1</span>
<span id="2">  2</span>
<span id="3">  3</span>
<span id="4">  4</span>
<span id="5">  5</span>
<span id="6">  6</span>
<span id="7">  7</span>
<span id="8">  8</span>
<span id="9">  9</span>
<span id="10"> 10</span>
<span id="11"> 11</span>
<span id="12"> 12</span>
<span id="13"> 13</span>
<span id="14"> 14</span>
<span id="15"> 15</span>
<span id="16"> 16</span>
<span id="17"> 17</span>
<span id="18"> 18</span>
<span id="19"> 19</span>
<span id="20"> 20</span>
<span id="21"> 21</span>
<span id="22"> 22</span>
<span id="23"> 23</span>
<span id="24"> 24</span>
<span id="25"> 25</span>
<span id="26"> 26</span>
<span id="27"> 27</span>
<span id="28"> 28</span>
<span id="29"> 29</span>
<span id="30"> 30</span>
<span id="31"> 31</span>
<span id="32"> 32</span>
<span id="33"> 33</span>
<span id="34"> 34</span>
<span id="35"> 35</span>
<span id="36"> 36</span>
<span id="37"> 37</span>
<span id="38"> 38</span>
<span id="39"> 39</span>
<span id="40"> 40</span>
<span id="41"> 41</span>
<span id="42"> 42</span>
<span id="43"> 43</span>
<span id="44"> 44</span>
<span id="45"> 45</span>
<span id="46"> 46</span>
<span id="47"> 47</span>
<span id="48"> 48</span>
<span id="49"> 49</span>
<span id="50"> 50</span>
<span id="51"> 51</span>
<span id="52"> 52</span>
<span id="53"> 53</span>
<span id="54"> 54</span>
<span id="55"> 55</span>
<span id="56"> 56</span>
<span id="57"> 57</span>
<span id="58"> 58</span>
<span id="59"> 59</span>
<span id="60"> 60</span>
<span id="61"> 61</span>
<span id="62"> 62</span>
<span id="63"> 63</span>
<span id="64"> 64</span>
<span id="65"> 65</span>
<span id="66"> 66</span>
<span id="67"> 67</span>
<span id="68"> 68</span>
<span id="69"> 69</span>
<span id="70"> 70</span>
<span id="71"> 71</span>
<span id="72"> 72</span>
<span id="73"> 73</span>
<span id="74"> 74</span>
<span id="75"> 75</span>
<span id="76"> 76</span>
<span id="77"> 77</span>
<span id="78"> 78</span>
<span id="79"> 79</span>
<span id="80"> 80</span>
<span id="81"> 81</span>
<span id="82"> 82</span>
<span id="83"> 83</span>
<span id="84"> 84</span>
<span id="85"> 85</span>
<span id="86"> 86</span>
<span id="87"> 87</span>
<span id="88"> 88</span>
<span id="89"> 89</span>
<span id="90"> 90</span>
<span id="91"> 91</span>
<span id="92"> 92</span>
<span id="93"> 93</span>
<span id="94"> 94</span>
<span id="95"> 95</span>
<span id="96"> 96</span>
<span id="97"> 97</span>
<span id="98"> 98</span>
<span id="99"> 99</span>
<span id="100">100</span>
<span id="101">101</span>
<span id="102">102</span>
<span id="103">103</span>
<span id="104">104</span>
<span id="105">105</span>
<span id="106">106</span>
<span id="107">107</span>
<span id="108">108</span>
<span id="109">109</span>
<span id="110">110</span>
<span id="111">111</span>
<span id="112">112</span>
<span id="113">113</span>
<span id="114">114</span>
<span id="115">115</span>
<span id="116">116</span>
<span id="117">117</span>
<span id="118">118</span>
<span id="119">119</span>
<span id="120">120</span>
<span id="121">121</span>
<span id="122">122</span>
<span id="123">123</span>
<span id="124">124</span>
<span id="125">125</span>
<span id="126">126</span>
<span id="127">127</span>
<span id="128">128</span>
<span id="129">129</span>
<span id="130">130</span>
<span id="131">131</span>
<span id="132">132</span>
<span id="133">133</span>
<span id="134">134</span>
<span id="135">135</span>
<span id="136">136</span>
<span id="137">137</span>
<span id="138">138</span>
<span id="139">139</span>
<span id="140">140</span>
<span id="141">141</span>
<span id="142">142</span>
<span id="143">143</span>
<span id="144">144</span>
<span id="145">145</span>
<span id="146">146</span>
<span id="147">147</span>
<span id="148">148</span>
<span id="149">149</span>
<span id="150">150</span>
<span id="151">151</span>
<span id="152">152</span>
<span id="153">153</span>
<span id="154">154</span>
<span id="155">155</span>
<span id="156">156</span>
<span id="157">157</span>
<span id="158">158</span>
<span id="159">159</span>
<span id="160">160</span>
<span id="161">161</span>
<span id="162">162</span>
<span id="163">163</span>
<span id="164">164</span>
<span id="165">165</span>
<span id="166">166</span>
<span id="167">167</span>
<span id="168">168</span>
<span id="169">169</span>
<span id="170">170</span>
<span id="171">171</span>
<span id="172">172</span>
<span id="173">173</span>
<span id="174">174</span>
<span id="175">175</span>
<span id="176">176</span>
<span id="177">177</span>
<span id="178">178</span>
<span id="179">179</span>
<span id="180">180</span>
<span id="181">181</span>
<span id="182">182</span>
<span id="183">183</span>
<span id="184">184</span>
<span id="185">185</span>
<span id="186">186</span>
<span id="187">187</span>
<span id="188">188</span>
<span id="189">189</span>
<span id="190">190</span>
</pre><pre class='rust '>
<span class='doccomment'>//! Provides the IBlas library trait for Collenchyma implementation.</span>

<span class='kw'>use</span> <span class='ident'>super</span>::<span class='ident'>binary</span>::<span class='ident'>IBlasBinary</span>;
<span class='kw'>use</span> <span class='ident'>super</span>::<span class='ident'>transpose</span>::<span class='op'>*</span>;
<span class='kw'>use</span> <span class='ident'>collenchyma</span>::<span class='ident'>binary</span>::<span class='ident'>IBinary</span>;
<span class='kw'>use</span> <span class='ident'>collenchyma</span>::<span class='ident'>tensor</span>::<span class='ident'>SharedTensor</span>;
<span class='kw'>use</span> <span class='ident'>collenchyma</span>::<span class='ident'>device</span>::<span class='ident'>DeviceType</span>;

<span class='doccomment'>/// Provides the functionality for a backend to support Basic Linear Algebra Subprogram operations.</span>
<span class='kw'>pub</span> <span class='kw'>trait</span> <span class='ident'>IBlas</span><span class='op'>&lt;</span><span class='ident'>F</span><span class='op'>&gt;</span> { }

<span class='doccomment'>/// Provides the asum operation.</span>
<span class='kw'>pub</span> <span class='kw'>trait</span> <span class='ident'>Asum</span><span class='op'>&lt;</span><span class='ident'>F</span><span class='op'>&gt;</span> {
    <span class='doccomment'>/// Computes the absolute sum of vector `x` with complete memory management.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Saves the result to `result`.</span>
    <span class='doccomment'>/// This is a Level 1 BLAS operation.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// For a no-memory managed version see `asum_plain`.</span>
    <span class='kw'>fn</span> <span class='ident'>asum</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>, <span class='ident'>x</span>: <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>F</span><span class='op'>&gt;</span>, <span class='ident'>result</span>: <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>F</span><span class='op'>&gt;</span>) <span class='op'>-&gt;</span> <span class='prelude-ty'>Result</span><span class='op'>&lt;</span>(), ::<span class='ident'>collenchyma</span>::<span class='ident'>error</span>::<span class='ident'>Error</span><span class='op'>&gt;</span>;

    <span class='doccomment'>/// Computes the absolute sum of vector `x` without any memory management.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Saves the result to `result`.</span>
    <span class='doccomment'>/// This is a Level 1 BLAS operation.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// *Attention*:&lt;br/&gt;</span>
    <span class='doccomment'>/// For a correct computation result, you need to manage the memory allocation and synchronization yourself.&lt;br/&gt;</span>
    <span class='doccomment'>/// For a memory managed version see `asum`.</span>
    <span class='kw'>fn</span> <span class='ident'>asum_plain</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>, <span class='ident'>x</span>: <span class='kw-2'>&amp;</span><span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>F</span><span class='op'>&gt;</span>, <span class='ident'>result</span>: <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>F</span><span class='op'>&gt;</span>) <span class='op'>-&gt;</span> <span class='prelude-ty'>Result</span><span class='op'>&lt;</span>(), ::<span class='ident'>collenchyma</span>::<span class='ident'>error</span>::<span class='ident'>Error</span><span class='op'>&gt;</span>;
}

<span class='doccomment'>/// Provides the axpy operation.</span>
<span class='kw'>pub</span> <span class='kw'>trait</span> <span class='ident'>Axpy</span><span class='op'>&lt;</span><span class='ident'>F</span><span class='op'>&gt;</span> {
    <span class='doccomment'>/// Computes a vector `x` times a constant `a` plus a vector `y` aka. `a * x + y` with complete memory management.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Saves the resulting vector back into `y`.</span>
    <span class='doccomment'>/// This is a Level 1 BLAS operation.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// For a no-memory managed version see `axpy_plain`.</span>
    <span class='kw'>fn</span> <span class='ident'>axpy</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>, <span class='ident'>a</span>: <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>F</span><span class='op'>&gt;</span>, <span class='ident'>x</span>: <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>F</span><span class='op'>&gt;</span>, <span class='ident'>y</span>: <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>F</span><span class='op'>&gt;</span>) <span class='op'>-&gt;</span> <span class='prelude-ty'>Result</span><span class='op'>&lt;</span>(), ::<span class='ident'>collenchyma</span>::<span class='ident'>error</span>::<span class='ident'>Error</span><span class='op'>&gt;</span>;

    <span class='doccomment'>/// Computes a vector `x` times a constant `a` plus a vector `y` aka. `a * x + y` without any memory management.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Saves the resulting vector back into `y`.</span>
    <span class='doccomment'>/// This is a Level 1 BLAS operation.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// *Attention*:&lt;br/&gt;</span>
    <span class='doccomment'>/// For a correct computation result, you need to manage the memory allocation and synchronization yourself.&lt;br/&gt;</span>
    <span class='doccomment'>/// For a memory managed version see `axpy`.</span>
    <span class='kw'>fn</span> <span class='ident'>axpy_plain</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>, <span class='ident'>a</span>: <span class='kw-2'>&amp;</span><span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>F</span><span class='op'>&gt;</span>, <span class='ident'>x</span>: <span class='kw-2'>&amp;</span><span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>F</span><span class='op'>&gt;</span>, <span class='ident'>y</span>: <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>F</span><span class='op'>&gt;</span>) <span class='op'>-&gt;</span> <span class='prelude-ty'>Result</span><span class='op'>&lt;</span>(), ::<span class='ident'>collenchyma</span>::<span class='ident'>error</span>::<span class='ident'>Error</span><span class='op'>&gt;</span>;
}

<span class='doccomment'>/// Provides the copy operation.</span>
<span class='kw'>pub</span> <span class='kw'>trait</span> <span class='ident'>Copy</span><span class='op'>&lt;</span><span class='ident'>F</span><span class='op'>&gt;</span> {
    <span class='doccomment'>/// Copies `x.len()` elements of vector `x` into vector `y` with complete memory management.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Saves the result to `y`.</span>
    <span class='doccomment'>/// This is a Level 1 BLAS operation.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// For a no-memory managed version see `copy_plain`.</span>
    <span class='kw'>fn</span> <span class='ident'>copy</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>, <span class='ident'>x</span>: <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>F</span><span class='op'>&gt;</span>, <span class='ident'>y</span>: <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>F</span><span class='op'>&gt;</span>) <span class='op'>-&gt;</span> <span class='prelude-ty'>Result</span><span class='op'>&lt;</span>(), ::<span class='ident'>collenchyma</span>::<span class='ident'>error</span>::<span class='ident'>Error</span><span class='op'>&gt;</span>;

    <span class='doccomment'>/// Copies `x.len()` elements of vector `x` into vector `y` without any memory management.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Saves the result to `y`.</span>
    <span class='doccomment'>/// This is a Level 1 BLAS operation.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// *Attention*:&lt;br/&gt;</span>
    <span class='doccomment'>/// For a correct computation result, you need to manage the memory allocation and synchronization yourself.&lt;br/&gt;</span>
    <span class='doccomment'>/// For a memory managed version see `copy`.</span>
    <span class='kw'>fn</span> <span class='ident'>copy_plain</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>, <span class='ident'>x</span>: <span class='kw-2'>&amp;</span><span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>F</span><span class='op'>&gt;</span>, <span class='ident'>y</span>: <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>F</span><span class='op'>&gt;</span>) <span class='op'>-&gt;</span> <span class='prelude-ty'>Result</span><span class='op'>&lt;</span>(), ::<span class='ident'>collenchyma</span>::<span class='ident'>error</span>::<span class='ident'>Error</span><span class='op'>&gt;</span>;
}

<span class='doccomment'>/// Provides the dot operation.</span>
<span class='kw'>pub</span> <span class='kw'>trait</span> <span class='ident'>Dot</span><span class='op'>&lt;</span><span class='ident'>F</span><span class='op'>&gt;</span> {
    <span class='doccomment'>/// Computes the [dot product][dot-product] over x and y with complete memory management.</span>
    <span class='doccomment'>/// [dot-product]: https://en.wikipedia.org/wiki/Dot_product</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Saves the resulting value into `result`.</span>
    <span class='doccomment'>/// This is a Level 1 BLAS operation.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// For a no-memory managed version see `dot_plain`.</span>
    <span class='kw'>fn</span> <span class='ident'>dot</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>, <span class='ident'>x</span>: <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>F</span><span class='op'>&gt;</span>, <span class='ident'>y</span>: <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>F</span><span class='op'>&gt;</span>, <span class='ident'>result</span>: <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>F</span><span class='op'>&gt;</span>) <span class='op'>-&gt;</span> <span class='prelude-ty'>Result</span><span class='op'>&lt;</span>(), ::<span class='ident'>collenchyma</span>::<span class='ident'>error</span>::<span class='ident'>Error</span><span class='op'>&gt;</span>;

    <span class='doccomment'>/// Computes the [dot product][dot-product] over x and y without any memory management.</span>
    <span class='doccomment'>/// [dot-product]: https://en.wikipedia.org/wiki/Dot_product</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Saves the resulting value into `result`.</span>
    <span class='doccomment'>/// This is a Level 1 BLAS operation.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// *Attention*:&lt;br/&gt;</span>
    <span class='doccomment'>/// For a correct computation result, you need to manage the memory allocation and synchronization yourself.&lt;br/&gt;</span>
    <span class='doccomment'>/// For a memory managed version see `dot`.</span>
    <span class='kw'>fn</span> <span class='ident'>dot_plain</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>, <span class='ident'>x</span>: <span class='kw-2'>&amp;</span><span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>F</span><span class='op'>&gt;</span>, <span class='ident'>y</span>: <span class='kw-2'>&amp;</span><span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>F</span><span class='op'>&gt;</span>, <span class='ident'>result</span>: <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>F</span><span class='op'>&gt;</span>) <span class='op'>-&gt;</span> <span class='prelude-ty'>Result</span><span class='op'>&lt;</span>(), ::<span class='ident'>collenchyma</span>::<span class='ident'>error</span>::<span class='ident'>Error</span><span class='op'>&gt;</span>;
}

<span class='doccomment'>/// Provides the nrm2 operation.</span>
<span class='kw'>pub</span> <span class='kw'>trait</span> <span class='ident'>Nrm2</span><span class='op'>&lt;</span><span class='ident'>F</span><span class='op'>&gt;</span> {
    <span class='doccomment'>/// Computes the L2 norm aka. euclidean length of vector `x` with complete memory management.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Saves the result to `result`.</span>
    <span class='doccomment'>/// This is a Level 1 BLAS operation.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// For a no-memory managed version see `nrm2_plain`.</span>
    <span class='kw'>fn</span> <span class='ident'>nrm2</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>, <span class='ident'>x</span>: <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>F</span><span class='op'>&gt;</span>, <span class='ident'>result</span>: <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>F</span><span class='op'>&gt;</span>) <span class='op'>-&gt;</span> <span class='prelude-ty'>Result</span><span class='op'>&lt;</span>(), ::<span class='ident'>collenchyma</span>::<span class='ident'>error</span>::<span class='ident'>Error</span><span class='op'>&gt;</span>;

    <span class='doccomment'>/// Computes the L2 norm aka. euclidean length of vector `x` without any memory management.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Saves the result to `result`.</span>
    <span class='doccomment'>/// This is a Level 1 BLAS operation.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// *Attention*:&lt;br/&gt;</span>
    <span class='doccomment'>/// For a correct computation result, you need to manage the memory allocation and synchronization yourself.&lt;br/&gt;</span>
    <span class='doccomment'>/// For a memory managed version see `nrm2`.</span>
    <span class='kw'>fn</span> <span class='ident'>nrm2_plain</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>, <span class='ident'>x</span>: <span class='kw-2'>&amp;</span><span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>F</span><span class='op'>&gt;</span>, <span class='ident'>result</span>: <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>F</span><span class='op'>&gt;</span>) <span class='op'>-&gt;</span> <span class='prelude-ty'>Result</span><span class='op'>&lt;</span>(), ::<span class='ident'>collenchyma</span>::<span class='ident'>error</span>::<span class='ident'>Error</span><span class='op'>&gt;</span>;
}

<span class='doccomment'>/// Provides the scal operation.</span>
<span class='kw'>pub</span> <span class='kw'>trait</span> <span class='ident'>Scal</span><span class='op'>&lt;</span><span class='ident'>F</span><span class='op'>&gt;</span> {
    <span class='doccomment'>/// Scales a vector `x` by a constant `a` aka. `a * x` with complete memory management.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Saves the resulting vector back into `x`.</span>
    <span class='doccomment'>/// This is a Level 1 BLAS operation.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// For a no-memory managed version see `scale_plain`.</span>
    <span class='kw'>fn</span> <span class='ident'>scal</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>, <span class='ident'>a</span>: <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>F</span><span class='op'>&gt;</span>, <span class='ident'>x</span>: <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>F</span><span class='op'>&gt;</span>) <span class='op'>-&gt;</span> <span class='prelude-ty'>Result</span><span class='op'>&lt;</span>(), ::<span class='ident'>collenchyma</span>::<span class='ident'>error</span>::<span class='ident'>Error</span><span class='op'>&gt;</span>;

    <span class='doccomment'>/// Scales a vector `x` by a constant `a` aka. `a * x` without any memory management.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Saves the resulting vector back into `x`.</span>
    <span class='doccomment'>/// This is a Level 1 BLAS operation.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// *Attention*:&lt;br/&gt;</span>
    <span class='doccomment'>/// For a correct computation result, you need to manage the memory allocation and synchronization yourself.&lt;br/&gt;</span>
    <span class='doccomment'>/// For a memory managed version see `scale`.</span>
    <span class='kw'>fn</span> <span class='ident'>scal_plain</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>, <span class='ident'>a</span>: <span class='kw-2'>&amp;</span><span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>F</span><span class='op'>&gt;</span>, <span class='ident'>x</span>: <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>F</span><span class='op'>&gt;</span>) <span class='op'>-&gt;</span> <span class='prelude-ty'>Result</span><span class='op'>&lt;</span>(), ::<span class='ident'>collenchyma</span>::<span class='ident'>error</span>::<span class='ident'>Error</span><span class='op'>&gt;</span>;
}

<span class='doccomment'>/// Provides the swap operation.</span>
<span class='kw'>pub</span> <span class='kw'>trait</span> <span class='ident'>Swap</span><span class='op'>&lt;</span><span class='ident'>F</span><span class='op'>&gt;</span> {
    <span class='doccomment'>/// Swaps the content of vector `x` and vector `y` with complete memory management.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Saves the resulting vector back into `x`.</span>
    <span class='doccomment'>/// This is a Level 1 BLAS operation.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// For a no-memory managed version see `swap_plain`.</span>
    <span class='kw'>fn</span> <span class='ident'>swap</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>, <span class='ident'>x</span>: <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>F</span><span class='op'>&gt;</span>, <span class='ident'>y</span>: <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>F</span><span class='op'>&gt;</span>) <span class='op'>-&gt;</span> <span class='prelude-ty'>Result</span><span class='op'>&lt;</span>(), ::<span class='ident'>collenchyma</span>::<span class='ident'>error</span>::<span class='ident'>Error</span><span class='op'>&gt;</span>;

    <span class='doccomment'>/// Swaps the content of vector `x` and vector `y` without any memory management.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Saves the resulting vector back into `x`.</span>
    <span class='doccomment'>/// This is a Level 1 BLAS operation.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// *Attention*:&lt;br/&gt;</span>
    <span class='doccomment'>/// For a correct computation result, you need to manage the memory allocation and synchronization yourself.&lt;br/&gt;</span>
    <span class='doccomment'>/// For a memory managed version see `swap`.</span>
    <span class='kw'>fn</span> <span class='ident'>swap_plain</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>, <span class='ident'>x</span>: <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>F</span><span class='op'>&gt;</span>, <span class='ident'>y</span>: <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>F</span><span class='op'>&gt;</span>) <span class='op'>-&gt;</span> <span class='prelude-ty'>Result</span><span class='op'>&lt;</span>(), ::<span class='ident'>collenchyma</span>::<span class='ident'>error</span>::<span class='ident'>Error</span><span class='op'>&gt;</span>;
}

<span class='doccomment'>/// Provides the gemm operation.</span>
<span class='kw'>pub</span> <span class='kw'>trait</span> <span class='ident'>Gemm</span><span class='op'>&lt;</span><span class='ident'>F</span><span class='op'>&gt;</span> {
    <span class='doccomment'>/// Computes a matrix-matrix product with general matrices.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Saves the result into `c`.</span>
    <span class='doccomment'>/// This is a Level 3 BLAS operation.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// For a no-memory managed version see `gemm_plain`.</span>
    <span class='kw'>fn</span> <span class='ident'>gemm</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>, <span class='ident'>alpha</span>: <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>F</span><span class='op'>&gt;</span>, <span class='ident'>at</span>: <span class='ident'>Transpose</span>, <span class='ident'>a</span>: <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>F</span><span class='op'>&gt;</span>, <span class='ident'>bt</span>: <span class='ident'>Transpose</span>, <span class='ident'>b</span>: <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>F</span><span class='op'>&gt;</span>, <span class='ident'>beta</span>: <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>F</span><span class='op'>&gt;</span>, <span class='ident'>c</span>: <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>F</span><span class='op'>&gt;</span>) <span class='op'>-&gt;</span> <span class='prelude-ty'>Result</span><span class='op'>&lt;</span>(), ::<span class='ident'>collenchyma</span>::<span class='ident'>error</span>::<span class='ident'>Error</span><span class='op'>&gt;</span>;

    <span class='doccomment'>/// Computes a matrix-matrix product with general matrices.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Saves the result into `c`.</span>
    <span class='doccomment'>/// This is a Level 3 BLAS operation.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// *Attention*:&lt;br/&gt;</span>
    <span class='doccomment'>/// For a correct computation result, you need to manage the memory allocation and synchronization yourself.&lt;br/&gt;</span>
    <span class='doccomment'>/// For a memory managed version see `gemm`.</span>
    <span class='kw'>fn</span> <span class='ident'>gemm_plain</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>, <span class='ident'>alpha</span>: <span class='kw-2'>&amp;</span><span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>F</span><span class='op'>&gt;</span>, <span class='ident'>at</span>: <span class='ident'>Transpose</span>, <span class='ident'>a</span>: <span class='kw-2'>&amp;</span><span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>F</span><span class='op'>&gt;</span>, <span class='ident'>bt</span>: <span class='ident'>Transpose</span>, <span class='ident'>b</span>: <span class='kw-2'>&amp;</span><span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>F</span><span class='op'>&gt;</span>, <span class='ident'>beta</span>: <span class='kw-2'>&amp;</span><span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>F</span><span class='op'>&gt;</span>, <span class='ident'>c</span>: <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>F</span><span class='op'>&gt;</span>) <span class='op'>-&gt;</span> <span class='prelude-ty'>Result</span><span class='op'>&lt;</span>(), ::<span class='ident'>collenchyma</span>::<span class='ident'>error</span>::<span class='ident'>Error</span><span class='op'>&gt;</span>;
}

<span class='doccomment'>/// Allows a BlasBinary to be provided which is used for a IBlas implementation.</span>
<span class='kw'>pub</span> <span class='kw'>trait</span> <span class='ident'>BlasBinaryProvider</span><span class='op'>&lt;</span><span class='ident'>F</span>, <span class='ident'>B</span>: <span class='ident'>IBlasBinary</span><span class='op'>&lt;</span><span class='ident'>F</span><span class='op'>&gt;</span> <span class='op'>+</span> <span class='ident'>IBinary</span><span class='op'>&gt;</span> {
    <span class='doccomment'>/// Returns the binary representation</span>
    <span class='kw'>fn</span> <span class='ident'>binary</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>) <span class='op'>-&gt;</span> <span class='kw-2'>&amp;</span><span class='ident'>B</span>;
    <span class='doccomment'>/// Returns the device representation</span>
    <span class='kw'>fn</span> <span class='ident'>device</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>) <span class='op'>-&gt;</span> <span class='kw-2'>&amp;</span><span class='ident'>DeviceType</span>;
}

<span class='kw'>impl</span><span class='op'>&lt;</span><span class='ident'>F</span>, <span class='ident'>B</span>: <span class='ident'>IBlasBinary</span><span class='op'>&lt;</span><span class='ident'>F</span><span class='op'>&gt;</span> <span class='op'>+</span> <span class='ident'>IBinary</span><span class='op'>&gt;</span> <span class='ident'>IBlas</span><span class='op'>&lt;</span><span class='ident'>F</span><span class='op'>&gt;</span> <span class='kw'>for</span> <span class='ident'>BlasBinaryProvider</span><span class='op'>&lt;</span><span class='ident'>F</span>, <span class='ident'>B</span><span class='op'>&gt;</span> { }
</pre>
</section>
    <section id='search' class="content hidden"></section>

    <section class="footer"></section>

    <aside id="help" class="hidden">
        <div>
            <h1 class="hidden">Help</h1>

            <div class="shortcuts">
                <h2>Keyboard Shortcuts</h2>

                <dl>
                    <dt>?</dt>
                    <dd>Show this help dialog</dd>
                    <dt>S</dt>
                    <dd>Focus the search field</dd>
                    <dt>&larrb;</dt>
                    <dd>Move up in search results</dd>
                    <dt>&rarrb;</dt>
                    <dd>Move down in search results</dd>
                    <dt>&#9166;</dt>
                    <dd>Go to active search result</dd>
                </dl>
            </div>

            <div class="infos">
                <h2>Search Tricks</h2>

                <p>
                    Prefix searches with a type followed by a colon (e.g.
                    <code>fn:</code>) to restrict the search to a given type.
                </p>

                <p>
                    Accepted types are: <code>fn</code>, <code>mod</code>,
                    <code>struct</code>, <code>enum</code>,
                    <code>trait</code>, <code>type</code>, <code>macro</code>,
                    and <code>const</code>.
                </p>

                <p>
                    Search functions by type signature (e.g.
                    <code>vec -> usize</code>)
                </p>
            </div>
        </div>
    </aside>

    

    <script>
        window.rootPath = "../../";
        window.currentCrate = "collenchyma_blas";
        window.playgroundUrl = "";
    </script>
    <script src="../../jquery.js"></script>
    <script src="../../main.js"></script>
    
    <script defer src="../../search-index.js"></script>
</body>
</html>