<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="generator" content="rustdoc">
    <meta name="description" content="Source to the Rust file `/home/travis/.cargo/registry/src/github.com-88ac128001ac3a9a/collenchyma-nn-0.3.4/src/lib.rs`.">
    <meta name="keywords" content="rust, rustlang, rust-lang">

    <title>lib.rs.html -- source</title>

    <link rel="stylesheet" type="text/css" href="../../rustdoc.css">
    <link rel="stylesheet" type="text/css" href="../../main.css">

    
    
</head>
<body class="rustdoc">
    <!--[if lte IE 8]>
    <div class="warning">
        This old browser is unsupported and will most likely display funky
        things.
    </div>
    <![endif]-->

    

    <nav class="sidebar">
        
        
    </nav>

    <nav class="sub">
        <form class="search-form js-only">
            <div class="search-container">
                <input class="search-input" name="search"
                       autocomplete="off"
                       placeholder="Click or press ‘S’ to search, ‘?’ for more options…"
                       type="search">
            </div>
        </form>
    </nav>

    <section id='main' class="content source"><pre class="line-numbers"><span id="1">  1</span>
<span id="2">  2</span>
<span id="3">  3</span>
<span id="4">  4</span>
<span id="5">  5</span>
<span id="6">  6</span>
<span id="7">  7</span>
<span id="8">  8</span>
<span id="9">  9</span>
<span id="10"> 10</span>
<span id="11"> 11</span>
<span id="12"> 12</span>
<span id="13"> 13</span>
<span id="14"> 14</span>
<span id="15"> 15</span>
<span id="16"> 16</span>
<span id="17"> 17</span>
<span id="18"> 18</span>
<span id="19"> 19</span>
<span id="20"> 20</span>
<span id="21"> 21</span>
<span id="22"> 22</span>
<span id="23"> 23</span>
<span id="24"> 24</span>
<span id="25"> 25</span>
<span id="26"> 26</span>
<span id="27"> 27</span>
<span id="28"> 28</span>
<span id="29"> 29</span>
<span id="30"> 30</span>
<span id="31"> 31</span>
<span id="32"> 32</span>
<span id="33"> 33</span>
<span id="34"> 34</span>
<span id="35"> 35</span>
<span id="36"> 36</span>
<span id="37"> 37</span>
<span id="38"> 38</span>
<span id="39"> 39</span>
<span id="40"> 40</span>
<span id="41"> 41</span>
<span id="42"> 42</span>
<span id="43"> 43</span>
<span id="44"> 44</span>
<span id="45"> 45</span>
<span id="46"> 46</span>
<span id="47"> 47</span>
<span id="48"> 48</span>
<span id="49"> 49</span>
<span id="50"> 50</span>
<span id="51"> 51</span>
<span id="52"> 52</span>
<span id="53"> 53</span>
<span id="54"> 54</span>
<span id="55"> 55</span>
<span id="56"> 56</span>
<span id="57"> 57</span>
<span id="58"> 58</span>
<span id="59"> 59</span>
<span id="60"> 60</span>
<span id="61"> 61</span>
<span id="62"> 62</span>
<span id="63"> 63</span>
<span id="64"> 64</span>
<span id="65"> 65</span>
<span id="66"> 66</span>
<span id="67"> 67</span>
<span id="68"> 68</span>
<span id="69"> 69</span>
<span id="70"> 70</span>
<span id="71"> 71</span>
<span id="72"> 72</span>
<span id="73"> 73</span>
<span id="74"> 74</span>
<span id="75"> 75</span>
<span id="76"> 76</span>
<span id="77"> 77</span>
<span id="78"> 78</span>
<span id="79"> 79</span>
<span id="80"> 80</span>
<span id="81"> 81</span>
<span id="82"> 82</span>
<span id="83"> 83</span>
<span id="84"> 84</span>
<span id="85"> 85</span>
<span id="86"> 86</span>
<span id="87"> 87</span>
<span id="88"> 88</span>
<span id="89"> 89</span>
<span id="90"> 90</span>
<span id="91"> 91</span>
<span id="92"> 92</span>
<span id="93"> 93</span>
<span id="94"> 94</span>
<span id="95"> 95</span>
<span id="96"> 96</span>
<span id="97"> 97</span>
<span id="98"> 98</span>
<span id="99"> 99</span>
<span id="100">100</span>
<span id="101">101</span>
<span id="102">102</span>
<span id="103">103</span>
<span id="104">104</span>
<span id="105">105</span>
<span id="106">106</span>
<span id="107">107</span>
<span id="108">108</span>
<span id="109">109</span>
<span id="110">110</span>
<span id="111">111</span>
<span id="112">112</span>
<span id="113">113</span>
<span id="114">114</span>
<span id="115">115</span>
<span id="116">116</span>
<span id="117">117</span>
<span id="118">118</span>
<span id="119">119</span>
<span id="120">120</span>
<span id="121">121</span>
<span id="122">122</span>
<span id="123">123</span>
<span id="124">124</span>
<span id="125">125</span>
</pre><pre class='rust '>
<span class='doccomment'>//! Provides a [Collenchyma][collenchyma] Plugin, to extend Collenchyma with Neural Network related</span>
<span class='doccomment'>//! operations such as convolutions, pooling, ReLU, etc. A full list of operations provided by this Plugin,</span>
<span class='doccomment'>//! can be found at the [provided Operations section](#operations).</span>
<span class='doccomment'>//!</span>
<span class='doccomment'>//! ## Overview</span>
<span class='doccomment'>//!</span>
<span class='doccomment'>//! This Collenchyma Plugin extends Collenchyma&#39;s Backend with NN related methods/operations. This allows</span>
<span class='doccomment'>//! you to run, these operations (and therefore your application) on your local machine as well as on servers,</span>
<span class='doccomment'>//! mobiles or any other machine (as if they were written for common CPU execution), while</span>
<span class='doccomment'>//! receiving the significant performance increases (usually one-to-two orders of magnitutde), by</span>
<span class='doccomment'>//! executing the operations on special purpose hardware such as GPUs - if they are available. Usage examples</span>
<span class='doccomment'>//! can be found in the next section.</span>
<span class='doccomment'>//!</span>
<span class='doccomment'>//! The architecture of a Plugin is quite easy. It defines one Plugin Trait, in this case the `NN`</span>
<span class='doccomment'>//! trait, which implements basic functionality for initialization and multiple Plugin Operation Traits which define the</span>
<span class='doccomment'>//! methods which are going to be available on the Backed, as the Plugin Trait as well as the Plugin Operations Traits</span>
<span class='doccomment'>//! are implemented for the Collenchyma Backends (CUDA, OpenCL, Native). The operations take as arguments one or many</span>
<span class='doccomment'>//! SharedTensors, holding the data over which the operation should happen, and none or one Operation Configuration.</span>
<span class='doccomment'>//!</span>
<span class='doccomment'>//! ## Usage</span>
<span class='doccomment'>//!</span>
<span class='doccomment'>//! An example on how to write some data into a SharedTensor and compute the result of the</span>
<span class='doccomment'>//! sigmoid function for each value:</span>
<span class='doccomment'>//!</span>
<span class='doccomment'>//! ```rust</span>
<span class='doccomment'>//! # #![allow(dead_code)]</span>
<span class='doccomment'>//! extern crate collenchyma as co;</span>
<span class='doccomment'>//! extern crate collenchyma_nn as nn;</span>
<span class='doccomment'>//! # #[cfg(feature = &quot;cuda&quot;)]</span>
<span class='doccomment'>//! # mod cuda {</span>
<span class='doccomment'>//! use co::prelude::*;</span>
<span class='doccomment'>//! use nn::*;</span>
<span class='doccomment'>//!</span>
<span class='doccomment'>//! fn write_to_memory&lt;T: Copy&gt;(mem: &amp;mut MemoryType, data: &amp;[T]) {</span>
<span class='doccomment'>//!     if let &amp;mut MemoryType::Native(ref mut mem) = mem {</span>
<span class='doccomment'>//!         let mut mem_buffer = mem.as_mut_slice::&lt;T&gt;();</span>
<span class='doccomment'>//!         for (index, datum) in data.iter().enumerate() {</span>
<span class='doccomment'>//!             mem_buffer[index] = *datum;</span>
<span class='doccomment'>//!         }</span>
<span class='doccomment'>//!     }</span>
<span class='doccomment'>//! }</span>
<span class='doccomment'>//!</span>
<span class='doccomment'>//! pub fn main() {</span>
<span class='doccomment'>//!     // Initialize a CUDA Backend.</span>
<span class='doccomment'>//!     // Usually you would not use CUDA but let Collenchyma pick what is available on the machine.</span>
<span class='doccomment'>//!     let backend = Backend::&lt;Cuda&gt;::default().unwrap();</span>
<span class='doccomment'>//!     // Initialize two SharedTensors.</span>
<span class='doccomment'>//!     let mut x = SharedTensor::&lt;f32&gt;::new(backend.device(), &amp;(1, 1, 3)).unwrap();</span>
<span class='doccomment'>//!     let mut result = SharedTensor::&lt;f32&gt;::new(backend.device(), &amp;(1, 1, 3)).unwrap();</span>
<span class='doccomment'>//!     // Fill `x` with some data.</span>
<span class='doccomment'>//!     let payload: &amp;[f32] = &amp;::std::iter::repeat(1f32).take(x.capacity()).collect::&lt;Vec&lt;f32&gt;&gt;();</span>
<span class='doccomment'>//!     let native = Native::new();</span>
<span class='doccomment'>//!     let cpu = native.new_device(native.hardwares()).unwrap();</span>
<span class='doccomment'>//!     x.add_device(&amp;cpu).unwrap(); // Add native host memory</span>
<span class='doccomment'>//!     x.sync(&amp;cpu).unwrap(); // Sync to native host memory</span>
<span class='doccomment'>//!     write_to_memory(x.get_mut(&amp;cpu).unwrap(), payload); // Write to native host memory.</span>
<span class='doccomment'>//!     x.sync(backend.device()).unwrap(); // Sync the data to the CUDA device.</span>
<span class='doccomment'>//!     // Run the sigmoid operation, provided by the NN Plugin, on your CUDA enabled GPU.</span>
<span class='doccomment'>//!     backend.sigmoid(&amp;mut x, &amp;mut result).unwrap();</span>
<span class='doccomment'>//!     // See the result.</span>
<span class='doccomment'>//!     result.add_device(&amp;cpu).unwrap(); // Add native host memory</span>
<span class='doccomment'>//!     result.sync(&amp;cpu).unwrap(); // Sync the result to host memory.</span>
<span class='doccomment'>//!     println!(&quot;{:?}&quot;, result.get(&amp;cpu).unwrap().as_native().unwrap().as_slice::&lt;f64&gt;());</span>
<span class='doccomment'>//! }</span>
<span class='doccomment'>//! # }</span>
<span class='doccomment'>//! # #[cfg(not(feature = &quot;cuda&quot;))]</span>
<span class='doccomment'>//! # mod cuda {</span>
<span class='doccomment'>//! # pub fn main() {}</span>
<span class='doccomment'>//! # }</span>
<span class='doccomment'>//! #</span>
<span class='doccomment'>//! # fn main() {</span>
<span class='doccomment'>//! #     if cfg!(feature = &quot;cuda&quot;) {</span>
<span class='doccomment'>//! #         ::cuda::main();</span>
<span class='doccomment'>//! #    }</span>
<span class='doccomment'>//! # }</span>
<span class='doccomment'>//! ```</span>
<span class='doccomment'>//!</span>
<span class='doccomment'>//! ## Provided Operations</span>
<span class='doccomment'>//!</span>
<span class='doccomment'>//! This Plugins provides the following operations. (Forward + Backward)</span>
<span class='doccomment'>//! A `-` means not yet implemented.</span>
<span class='doccomment'>//!</span>
<span class='doccomment'>//! | Operation            | CUDA       | OpenCL    | Native    |</span>
<span class='doccomment'>//! |---	               |---	        |---        |---        |</span>
<span class='doccomment'>//! | Sigmoid  	           | cuDNN v3  	| -  	    | Rust  	|</span>
<span class='doccomment'>//! | SigmoidPointwise     | cuDNN v3  	| -  	    |   	    |</span>
<span class='doccomment'>//! | ReLU  	           | cuDNN v3   | -  	    | Rust 	    |</span>
<span class='doccomment'>//! | ReLUPointwise        | cuDNN v3  	| -  	    |   	    |</span>
<span class='doccomment'>//! | Tanh  	   	       | cudNN v3   | - 	    | Rust      |</span>
<span class='doccomment'>//! | TanhPointwise        | cuDNN v3  	| -  	    |   	    |</span>
<span class='doccomment'>//! |   	   	           |  	        |  	        |           |</span>
<span class='doccomment'>//! | Normalization (LRN)  | cudNN v3   | - 	    | -         |</span>
<span class='doccomment'>//! |   	   	           |  	        |  	        |           |</span>
<span class='doccomment'>//! | Convolution          | cudNN v3   | - 	    | -         |</span>
<span class='doccomment'>//! |   	   	           |  	        |  	        |           |</span>
<span class='doccomment'>//! | Softmax              | cudNN v3   | - 	    | Rust      |</span>
<span class='doccomment'>//! | LogSoftmax           | cudNN v3   | - 	    | Rust      |</span>
<span class='doccomment'>//! |   	   	           |  	        |  	        |           |</span>
<span class='doccomment'>//! | Pooling Max          | cudNN v3   | - 	    | -         |</span>
<span class='doccomment'>//! | Pooling Avg          | cudNN v3   | - 	    | -         |</span>
<span class='doccomment'>//!</span>
<span class='doccomment'>//! [collenchyma]: https://github.com/autumnai/collenchyma</span>
<span class='doccomment'>//! [collenchyma-docs]: http://autumnai.github.io/collenchyma</span>
<span class='doccomment'>//! [blas-source]: https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms</span>
<span class='attribute'>#<span class='op'>!</span>[<span class='ident'>cfg_attr</span>(<span class='ident'>lint</span>, <span class='ident'>feature</span>(<span class='ident'>plugin</span>))]</span>
<span class='attribute'>#<span class='op'>!</span>[<span class='ident'>cfg_attr</span>(<span class='ident'>lint</span>, <span class='ident'>plugin</span>(<span class='ident'>clippy</span>))]</span>
<span class='attribute'>#<span class='op'>!</span>[<span class='ident'>allow</span>(<span class='ident'>dead_code</span>)]</span>
<span class='attribute'>#<span class='op'>!</span>[<span class='ident'>deny</span>(<span class='ident'>missing_docs</span>,
        <span class='ident'>missing_debug_implementations</span>, <span class='ident'>missing_copy_implementations</span>,
        <span class='ident'>trivial_casts</span>, <span class='ident'>trivial_numeric_casts</span>,
        <span class='ident'>unused_import_braces</span>, <span class='ident'>unused_qualifications</span>)]</span>

<span class='kw'>extern</span> <span class='kw'>crate</span> <span class='ident'>collenchyma</span> <span class='kw'>as</span> <span class='ident'>co</span>;
<span class='attribute'>#[<span class='ident'>cfg</span>(<span class='ident'>feature</span> <span class='op'>=</span> <span class='string'>&quot;cuda&quot;</span>)]</span>
<span class='kw'>extern</span> <span class='kw'>crate</span> <span class='ident'>cudnn</span>;
<span class='kw'>extern</span> <span class='kw'>crate</span> <span class='ident'>libc</span>;
<span class='attribute'>#[<span class='ident'>macro_use</span>]</span>
<span class='kw'>extern</span> <span class='kw'>crate</span> <span class='ident'>lazy_static</span>;
<span class='attribute'>#[<span class='ident'>macro_use</span>]</span>
<span class='kw'>extern</span> <span class='kw'>crate</span> <span class='ident'>log</span>;

<span class='kw'>pub</span> <span class='kw'>use</span> <span class='ident'>plugin</span>::<span class='op'>*</span>;

<span class='kw'>mod</span> <span class='ident'>plugin</span>;
<span class='kw'>pub</span> <span class='kw'>mod</span> <span class='ident'>frameworks</span>;
</pre>
</section>
    <section id='search' class="content hidden"></section>

    <section class="footer"></section>

    <aside id="help" class="hidden">
        <div>
            <h1 class="hidden">Help</h1>

            <div class="shortcuts">
                <h2>Keyboard Shortcuts</h2>

                <dl>
                    <dt>?</dt>
                    <dd>Show this help dialog</dd>
                    <dt>S</dt>
                    <dd>Focus the search field</dd>
                    <dt>&larrb;</dt>
                    <dd>Move up in search results</dd>
                    <dt>&rarrb;</dt>
                    <dd>Move down in search results</dd>
                    <dt>&#9166;</dt>
                    <dd>Go to active search result</dd>
                </dl>
            </div>

            <div class="infos">
                <h2>Search Tricks</h2>

                <p>
                    Prefix searches with a type followed by a colon (e.g.
                    <code>fn:</code>) to restrict the search to a given type.
                </p>

                <p>
                    Accepted types are: <code>fn</code>, <code>mod</code>,
                    <code>struct</code>, <code>enum</code>,
                    <code>trait</code>, <code>type</code>, <code>macro</code>,
                    and <code>const</code>.
                </p>

                <p>
                    Search functions by type signature (e.g.
                    <code>vec -> usize</code>)
                </p>
            </div>
        </div>
    </aside>

    

    <script>
        window.rootPath = "../../";
        window.currentCrate = "collenchyma_nn";
        window.playgroundUrl = "";
    </script>
    <script src="../../jquery.js"></script>
    <script src="../../main.js"></script>
    
    <script defer src="../../search-index.js"></script>
</body>
</html>